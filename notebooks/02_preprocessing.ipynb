{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import math\n",
    "import string\n",
    "from typing import Dict, List\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import emoji\n",
    "import pandas as pd\n",
    "from googletrans import Translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the directory where your datasets are stored\n",
    "raw_dir = '../data/raw/'\n",
    "middle_dir = '../data/middle/'\n",
    "final_dir = '../data/final/'\n",
    "\n",
    "# Create the middle directory if it doesn't exist\n",
    "os.makedirs(middle_dir, exist_ok=True)\n",
    "os.makedirs(final_dir, exist_ok=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## classla_FRENK-hate-en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text: str) -> str:\n",
    "    # Remove escape characters\n",
    "    text = text.replace('\\r', '').replace('\\n', '')\n",
    "    return text.strip()\n",
    "\n",
    "def preprocess_func1(file_path: str) -> pd.DataFrame:\n",
    "    # Télécharger dataset\n",
    "    data = pd.read_csv(file_path, sep='\\t', encoding='utf-8')\n",
    "\n",
    "    # Enlever missing values\n",
    "    if data.isna().sum().any() > 0:\n",
    "        data = data.dropna()\n",
    "\n",
    "    # Drop duplicates\n",
    "    if data.duplicated().sum() > 0:\n",
    "        data = data.drop_duplicates()\n",
    "\n",
    "    # Preprocess 'tweet' column\n",
    "    data['text'] = data['text'].apply(clean_text)\n",
    "\n",
    "    # Keep only specified columns\n",
    "    columns_to_keep = [\"label\", \"text\"]\n",
    "    data = data[columns_to_keep]\n",
    "\n",
    "    # Mettre en ordre columns\n",
    "    data = data[['label', 'text']]\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the three dataset file names\n",
    "dataset_files = [\n",
    "    'classla_FRENK-hate-en_test.tsv',\n",
    "    'classla_FRENK-hate-en_train.tsv',\n",
    "    'classla_FRENK-hate-en_validation.tsv'\n",
    "]\n",
    "\n",
    "for file_name in dataset_files:\n",
    "    file_path = os.path.join(raw_dir, file_name)\n",
    "\n",
    "    # Preprocess the dataset\n",
    "    preprocessed_data = preprocess_func1(file_path)\n",
    "\n",
    "    # Save the preprocessed data for each dataset\n",
    "    preprocessed_file_path = os.path.join(middle_dir, file_name)\n",
    "    preprocessed_data.to_csv(\n",
    "        preprocessed_file_path,\n",
    "        sep='\\t',\n",
    "        encoding='utf-8',\n",
    "        index=False\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hate_speech_offensive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweet(tweet: str) -> str:\n",
    "    # Remove intro section of tweet\n",
    "    tweet = re.sub(r'^.*?:', '', tweet)\n",
    "    # Remove escape characters\n",
    "    tweet = tweet.replace('\\r', '').replace('\\n', '')\n",
    "    # Remove quotes\n",
    "    tweet = tweet.replace('\"', '')\n",
    "    # Remove @user_name\n",
    "    tweet = re.sub(r'\\@\\w+', '', tweet)\n",
    "    # Remove url links\n",
    "    tweet = re.sub(r'//*', '', tweet)\n",
    "    return tweet.strip()\n",
    "\n",
    "def preprocess_func2(file_path: str) -> pd.DataFrame:\n",
    "    # Load dataset\n",
    "    data = pd.read_csv(file_path, sep='\\t', encoding='utf-8')\n",
    "\n",
    "    # Remove missing values\n",
    "    data = data.dropna()\n",
    "\n",
    "    # Drop duplicates\n",
    "    data = data.drop_duplicates()\n",
    "\n",
    "    # Preprocess 'tweet' column\n",
    "    data['text'] = data['tweet'].apply(clean_tweet)\n",
    "\n",
    "    # Keep only specified columns\n",
    "    data = data[['class', 'text']]\n",
    "\n",
    "    # Rename 'class' to 'label'\n",
    "    data = data.rename(columns={'class': 'label'})\n",
    "\n",
    "    # Replace values in the 'label' column: 0 -> 1, 2 -> 0\n",
    "    data['label'] = data['label'].replace({0: 1, 2: 0})\n",
    "\n",
    "    return data    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enregistrer\n",
    "file_name = 'hate_speech_offensive_train.tsv'\n",
    "file_path = os.path.join(raw_dir, file_name)\n",
    "\n",
    "# Preprocess the dataset\n",
    "preprocessed_data = preprocess_func2(file_path)\n",
    "\n",
    "# Save the preprocessed data for each dataset\n",
    "preprocessed_file_path = os.path.join(middle_dir, file_name)\n",
    "preprocessed_data.to_csv(\n",
    "    preprocessed_file_path,\n",
    "    sep='\\t',\n",
    "    encoding='utf-8',\n",
    "    index=False\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hate_speech18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text: str) -> str:\n",
    "    # Remove escape characters\n",
    "    text = text.replace('\\r', '').replace('\\n', '')\n",
    "    return text.strip()\n",
    "\n",
    "def preprocess_func3(file_path: str) -> pd.DataFrame:\n",
    "    # Télécharger dataset\n",
    "    data = pd.read_csv(file_path, sep='\\t', encoding='utf-8')\n",
    "\n",
    "    # Enlever missing values\n",
    "    if data.isna().sum().any() > 0:\n",
    "        data = data.dropna()\n",
    "\n",
    "    # Drop duplicates\n",
    "    if data.duplicated().sum() > 0:\n",
    "        data = data.drop_duplicates()\n",
    "    \n",
    "    # Preprocess 'tweet' column\n",
    "    data['text'] = data['text'].apply(clean_text)\n",
    "\n",
    "    # Keep only specified columns\n",
    "    columns_to_keep = [\"label\", \"text\"]\n",
    "    data = data[columns_to_keep]\n",
    "\n",
    "    # Mettre en ordre columns\n",
    "    data = data[['label', 'text']]\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enregistrer\n",
    "file_name = 'hate_speech18_train.tsv'\n",
    "file_path = os.path.join(raw_dir, file_name)\n",
    "\n",
    "# Preprocess the dataset\n",
    "preprocessed_data = preprocess_func3(file_path)\n",
    "\n",
    "# Save the preprocessed data for each dataset\n",
    "preprocessed_file_path = os.path.join(middle_dir, file_name)\n",
    "preprocessed_data.to_csv(\n",
    "    preprocessed_file_path,\n",
    "    sep='\\t',\n",
    "    encoding='utf-8',\n",
    "    index=False\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### limjiayi_hateful_memes_expanded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_func4(file_path: str) -> pd.DataFrame:\n",
    "    # Télécharger le dataset\n",
    "    data = pd.read_csv(file_path, sep='\\t', encoding='utf-8')\n",
    "\n",
    "    # Enlever les valeurs manquantes\n",
    "    if data.isna().sum().any() > 0:\n",
    "        data = data.dropna()\n",
    "\n",
    "    # Supprimer les doublons\n",
    "    if data.duplicated().sum() > 0:\n",
    "        data = data.drop_duplicates()\n",
    "\n",
    "    # Conserver seulement les colonnes spécifiées\n",
    "    columns_to_keep = [\"text\",\"label\"]\n",
    "    data = data[columns_to_keep]\n",
    "\n",
    "    # Ordre des colonnes : label, text\n",
    "    data = data[['label', 'text']]\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the three dataset file names\n",
    "dataset_files = [\n",
    "    'limjiayi_hateful_memes_expanded_train.tsv',\n",
    "    'limjiayi_hateful_memes_expanded_test.tsv',\n",
    "    'limjiayi_hateful_memes_expanded_validation.tsv'\n",
    "]\n",
    "\n",
    "for file_name in dataset_files:\n",
    "    file_path = os.path.join(raw_dir, file_name)\n",
    "\n",
    "    # Preprocess the dataset\n",
    "    preprocessed_data = preprocess_func4(file_path)\n",
    "\n",
    "    # Save the preprocessed data for each dataset\n",
    "    preprocessed_file_path = os.path.join(middle_dir, file_name)\n",
    "    preprocessed_data.to_csv(\n",
    "        preprocessed_file_path,\n",
    "        sep='\\t',\n",
    "        encoding='utf-8',\n",
    "        index=False\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tweets_hate_speech_detection_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweet(text: str) -> str:\n",
    "    # Remove escape characters\n",
    "    text = text.replace('\\r', '').replace('\\n', '')\n",
    "    return text.strip()\n",
    "\n",
    "def preprocess_func5(file_path: str) -> str:\n",
    "    # Télécharger le dataset\n",
    "    data = pd.read_csv(file_path, sep='\\t', encoding='utf-8')\n",
    "    \n",
    "    # Enlever les valeurs manquantes\n",
    "    if data.isna().sum().any() > 0:\n",
    "        data = data.dropna()\n",
    "\n",
    "    # Supprimer les doublons\n",
    "    if data.duplicated().sum() > 0:\n",
    "        data = data.drop_duplicates()\n",
    "\n",
    "    # Preprocess 'tweet' column\n",
    "    data['tweet'] = data['tweet'].apply(clean_tweet)\n",
    "    \n",
    "    # Renommer les colonnes\n",
    "    data = data.rename(columns={'tweet': 'text', 'label': 'label'})\n",
    "\n",
    "    # Ordre des colonnes : label, text\n",
    "    data = data[['label', 'text']]\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the three dataset file names\n",
    "dataset_files = [\n",
    "    'tweets_hate_speech_detection_train.tsv',\n",
    "    'tweets_hate_speech_detection_test.tsv'\n",
    "]\n",
    "\n",
    "for file_name in dataset_files:\n",
    "    file_path = os.path.join(raw_dir, file_name)\n",
    "\n",
    "    # Preprocess the dataset\n",
    "    preprocessed_data = preprocess_func5(file_path)\n",
    "\n",
    "    # Save the preprocessed data for each dataset\n",
    "    preprocessed_file_path = os.path.join(middle_dir, file_name)\n",
    "    preprocessed_data.to_csv(\n",
    "        preprocessed_file_path,\n",
    "        sep='\\t',\n",
    "        encoding='utf-8',\n",
    "        index=False\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ucberkeley-dlab_measuring-hate-speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_func6(file_path):\n",
    "    # Charger le dataset\n",
    "    data = pd.read_csv(file_path, sep='\\t', encoding='utf-8')\n",
    "\n",
    "    # Vérifier les valeurs manquantes\n",
    "    data.isna().sum()\n",
    "\n",
    "    # Vérifier les doublons\n",
    "    data.duplicated().sum()\n",
    "\n",
    "    # Conserver seulement les colonnes nécessaires\n",
    "    columns_to_keep = [\"hate_speech_score\", \"text\"]\n",
    "    data = data[columns_to_keep]\n",
    "\n",
    "    # Créer une colonne 'label' basée sur hate_speech_score\n",
    "    # rappel hate_speech_score - continuous hate speech measure,\n",
    "    # where higher = more hateful and lower = less hateful. > 0.5 is approximately hate speech,\n",
    "    # < -1 is counter or supportive speech, and -1 to +0.5 is neutral or ambiguous.\n",
    "    data['label'] = 0  # Initialise toutes les valeurs à 0\n",
    "\n",
    "    # Définir les conditions pour lesquelles hate_speech_score est considéré comme \"hateful\"\n",
    "    condition_hateful = data['hate_speech_score'] > 0\n",
    "\n",
    "    # Affecter 1 aux lignes où la condition est vraie\n",
    "    data.loc[condition_hateful, 'label'] = 1\n",
    "\n",
    "    # Supprimer la colonne 'hate_speech_score'\n",
    "    data = data.drop(['hate_speech_score'], axis=1)\n",
    "\n",
    "    # Sélectionner les colonnes 'label' et 'text'\n",
    "    data_cam = data[['label', 'text']]\n",
    "\n",
    "    return data_cam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enregistrer\n",
    "file_name = 'ucberkeley-dlab_measuring-hate-speech_train.tsv'\n",
    "file_path = os.path.join(raw_dir, file_name)\n",
    "\n",
    "# Preprocess the dataset\n",
    "preprocessed_data = preprocess_func6(file_path)\n",
    "\n",
    "# Save the preprocessed data for each dataset\n",
    "preprocessed_file_path = os.path.join(middle_dir, file_name)\n",
    "preprocessed_data.to_csv(\n",
    "    preprocessed_file_path,\n",
    "    sep='\\t',\n",
    "    encoding='utf-8',\n",
    "    index=False\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paul_hatecheck-french"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_func7(file_path: str) -> pd.DataFrame:\n",
    "    # Charger le dataset\n",
    "\n",
    "    data = pd.read_csv(file_path, sep='\\t', encoding='utf-8')\n",
    "    # Vérifier les valeurs manquantes\n",
    "    data.isna().sum()\n",
    "\n",
    "    # Vérifier les doublons\n",
    "    data.duplicated().sum()\n",
    "\n",
    "    # Conserver seulement les colonnes nécessaires\n",
    "    columns_to_keep = ['test_case', 'label_annotated_maj']\n",
    "    data = data[columns_to_keep]\n",
    "\n",
    "    # Renommer les colones\n",
    "    data = data.rename(columns={'test_case': 'text', 'label_annotated_maj': 'label'})\n",
    "\n",
    "    # Créer une nouvelle colonne 'label' avec 0 ou 1\n",
    "    data['label'] = data.apply(lambda row: 1 if row['label'] == 'hateful' else 1, axis=1)\n",
    "\n",
    "    # Ordre des colonnes : label, text\n",
    "    data = data[['label', 'text']]\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enregistrer\n",
    "file_name = 'Paul_hatecheck-french_test.tsv'\n",
    "file_path = os.path.join(raw_dir, file_name)\n",
    "\n",
    "# Preprocess the dataset\n",
    "preprocessed_data = preprocess_func7(file_path)\n",
    "\n",
    "# Save the preprocessed data for each dataset\n",
    "preprocessed_file_path = os.path.join(middle_dir, file_name)\n",
    "preprocessed_data.to_csv(\n",
    "    preprocessed_file_path,\n",
    "    sep='\\t',\n",
    "    encoding='utf-8',\n",
    "    index=False\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hatexplain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text: str) -> str:\n",
    "    # Remove escape characters\n",
    "    text = text.replace('\\r', '').replace('\\n', '')\n",
    "    return text.strip()\n",
    "\n",
    "def preprocess_func8(file_path: str) -> str:\n",
    "    # Télécharger le dataset\n",
    "    data = pd.read_csv(file_path, sep='\\t')\n",
    "\n",
    "    # Enlever les valeurs manquantes\n",
    "    if data.isna().sum().any() > 0:\n",
    "        data = data.dropna()\n",
    "\n",
    "    # Supprimer les doublons\n",
    "    if data.duplicated().sum() > 0:\n",
    "        data = data.drop_duplicates()\n",
    "\n",
    "    # Conserver seulement les colonnes spécifiées\n",
    "    columns_to_keep = [\"post_tokens\"]\n",
    "    data = data[columns_to_keep]\n",
    "\n",
    "    # Nettoyer les caractères spéciaux dans la colonne 'post_tokens'\n",
    "    data['text'] = data['post_tokens'].apply(lambda x: re.sub(r'<user>', '', x))\n",
    "    data['text'] = data['text'].apply(lambda x: re.sub(r'<utilisateur>', '', x))\n",
    "    data['text'] = data['text'].apply(lambda x: re.sub(r'[^a-zA-Z\\s]', '', x))\n",
    "\n",
    "    # Preprocess 'tweet' column\n",
    "    data['text'] = data['text'].apply(clean_tweet)\n",
    "\n",
    "    # Ajouter une colonne 'label' avec la valeur 1\n",
    "    data['label'] = 1\n",
    "\n",
    "    # Ordre des colonnes : label, text\n",
    "    data = data[['label', 'text']]\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the three dataset file names\n",
    "dataset_files = [\n",
    "    'hatexplain_train.tsv',\n",
    "    'hatexplain_test.tsv',\n",
    "    'hatexplain_validation.tsv'\n",
    "]\n",
    "\n",
    "for file_name in dataset_files:\n",
    "    file_path = os.path.join(raw_dir, file_name)\n",
    "\n",
    "    # Preprocess the dataset\n",
    "    preprocessed_data = preprocess_func8(file_path)\n",
    "\n",
    "    # Save the preprocessed data for each dataset\n",
    "    preprocessed_file_path = os.path.join(middle_dir, file_name)\n",
    "    preprocessed_data.to_csv(\n",
    "        preprocessed_file_path,\n",
    "        sep='\\t',\n",
    "        encoding='utf-8',\n",
    "        index=False\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenation and Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the dataframes from middle\n",
    "root_data = '../data/middle'\n",
    "files = [os.path.join(root_data, f) for f in os.listdir(root_data)]\n",
    "dfs = [pd.read_csv(file, sep='\\t', encoding='utf-8') for file in files]\n",
    "concat_df = pd.concat(dfs, ignore_index=True, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translator function\n",
    "def google_translate(text: str or List[str], src_lang:str = 'en', dest_lang: str = 'fr') -> str: # type: ignore\n",
    "  translation = Translator().translate(text=text, src=src_lang, dest=dest_lang)\n",
    "  if isinstance(text, list):\n",
    "    return [t.text for t in translation]\n",
    "  else:\n",
    "    return translation.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err_id = []\n",
    "for index, row in tqdm(concat_df.iterrows(), total=concat_df.shape[0]):\n",
    "    try:\n",
    "        concat_df.at[index, 'text'] = google_translate(\n",
    "            text=str(row['text']),\n",
    "            src_lang=\"en\",\n",
    "            dest_lang=\"fr\"\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print('Row', index, ':', e)\n",
    "        err_id.append(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Original length: {len(concat_df)}')\n",
    "concat_df = concat_df.drop(index=err_id)\n",
    "print(f'New length: {len(concat_df)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Cleaning & Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define list of acceptable characters\n",
    "# Adapted from https://github.com/mindee/doctr/blob/main/doctr/datasets/vocabs.py\n",
    "VOCABS: Dict[str, str] = {\n",
    "    \"digits\": string.digits,\n",
    "    \"ascii_letters\": string.ascii_letters,\n",
    "    \"punctuation\": string.punctuation,\n",
    "    \"whitespaces\": ' ',\n",
    "    \"emoji\": str(list(emoji.EMOJI_DATA)),\n",
    "    \"currency\": \"£€¥¢฿\",\n",
    "    \"ancient_greek\": \"αβγδεζηθικλμνξοπρστυφχψωΑΒΓΔΕΖΗΘΙΚΛΜΝΞΟΠΡΣΤΥΦΧΨΩ\",\n",
    "    \"arabic_letters\": \"ءآأؤإئابةتثجحخدذرزسشصضطظعغـفقكلمنهوىي\",\n",
    "    \"persian_letters\": \"پچڢڤگ\",\n",
    "    \"hindi_digits\": \"٠١٢٣٤٥٦٧٨٩\",\n",
    "    \"arabic_diacritics\": \"\",\n",
    "    \"arabic_punctuation\": \"؟؛«»—\",\n",
    "}\n",
    "VOCABS[\"latin\"] = VOCABS[\"digits\"] + VOCABS[\"ascii_letters\"] + VOCABS[\"punctuation\"] + VOCABS[\"whitespaces\"]\n",
    "VOCABS[\"legacy_french\"] = VOCABS[\"latin\"] + \"°\" + \"àâéèêëîïôùûçÀÂÉÈËÎÏÔÙÛÇ\" + VOCABS[\"currency\"] + VOCABS['emoji']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning function\n",
    "def text_cleaning(text:str, vocab: str) -> str:\n",
    "    for char in text:\n",
    "        # Check if character is in the acceptable voacbulary\n",
    "        if char not in vocab:\n",
    "            text = text.replace(char, '')\n",
    "        text = text.replace('\"', '')\n",
    "    return text.strip() # Remove trailing whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the dataframe\n",
    "vocab = VOCABS[\"legacy_french\"]\n",
    "for index, row in tqdm(concat_df.iterrows(), total=len(concat_df)):\n",
    "    if int(row['label']) == 0:\n",
    "        concat_df.at[index,'label'] = 0\n",
    "    else:\n",
    "        concat_df.at[index,'label'] = 1\n",
    "    try:\n",
    "        concat_df.at[index,'text'] = text_cleaning(str(row['text']), vocab)\n",
    "    except:\n",
    "        concat_df.at[index,'text'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original length: 267842\n",
      "Is there null values: False\n",
      "Final length: 267841\n"
     ]
    }
   ],
   "source": [
    "empty_indices = concat_df[concat_df['text'] == ''].index\n",
    "print(f'Original length: {len(concat_df)}')\n",
    "concat_df.drop(empty_indices, axis=0, inplace=True)\n",
    "print(f'Is there null values: {concat_df.isnull().values.any()}')\n",
    "clean_df = concat_df.dropna(how='any',axis=0) \n",
    "print(f'Final length: {len(clean_df)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of train set: 241056\n",
      "Size of test set: 26785\n"
     ]
    }
   ],
   "source": [
    "train_split = 0.9\n",
    "SEED_VAL = 999\n",
    "# Shuffle data\n",
    "clean_df_shuffle = clean_df.sample(frac=1, random_state=SEED_VAL).reset_index(drop=True)\n",
    "id = math.floor(len(clean_df_shuffle)*train_split)\n",
    "train_df = clean_df_shuffle[0:id]\n",
    "test_df = clean_df_shuffle[id:]\n",
    "print(f'Size of train set: {len(train_df)}')\n",
    "print(f'Size of test set: {len(test_df)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv('../data/final/train_data.tsv', sep='\\t', encoding='utf-8', index=False)\n",
    "test_df.to_csv('../data/final/text_data.tsv', sep='\\t', encoding='utf-8', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mad-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
